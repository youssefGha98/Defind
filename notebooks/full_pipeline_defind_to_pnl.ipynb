{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DefInd to PnL Full Pipeline\n",
    "\n",
    "This notebook demonstrates the complete pipeline from DefInd indexed data to PnL calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'ShardAggregator' from 'defind.storage.shards' (/home/youssef/define/src/defind/storage/shards.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# DefInd imports\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdefind\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mstorage\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmanifest\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LiveManifest\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdefind\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mstorage\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mshards\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ShardAggregator\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# Define imports\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdefine\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfig\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_settings\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'ShardAggregator' from 'defind.storage.shards' (/home/youssef/define/src/defind/storage/shards.py)"
     ]
    }
   ],
   "source": [
    "# Setup and imports\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "sys.path.append('../../src')  # For defind\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import duckdb\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "# DefInd imports\n",
    "from defind.storage.manifest import LiveManifest\n",
    "from defind.storage.shards import ShardAggregator\n",
    "\n",
    "# Define imports\n",
    "from define.config import load_settings\n",
    "from define.adapters import DuckDBRepository\n",
    "from define.application import PNLEngine, to_daily\n",
    "from define.domain import Pool, Token, PositionStatic, PositionEventState\n",
    "\n",
    "print(\"‚úÖ All imports successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Exploring DefInd Data\n",
      "========================================\n",
      "‚ö†Ô∏è No DefInd parquet files found. You may need to run DefInd first:\n",
      "   defind fetch-logs --registry nfpm --start-block 18000000 --end-block 18001000\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Explore DefInd Data\n",
    "print(\"üîç Exploring DefInd Data\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Look for DefInd data files\n",
    "data_paths = [\n",
    "    Path(\"../data\"),\n",
    "    Path(\"../../data\"),\n",
    "    Path(\"../src/defind/data/shards\"),\n",
    "    Path(\"../../src/defind/data/shards\")\n",
    "]\n",
    "\n",
    "found_data = []\n",
    "for path in data_paths:\n",
    "    if path.exists():\n",
    "        parquet_files = list(path.glob(\"*.parquet\"))\n",
    "        if parquet_files:\n",
    "            found_data.extend(parquet_files)\n",
    "            print(f\"Found {len(parquet_files)} parquet files in {path}\")\n",
    "\n",
    "if found_data:\n",
    "    print(f\"\\nTotal parquet files found: {len(found_data)}\")\n",
    "    for f in found_data[:5]:  # Show first 5\n",
    "        print(f\"  - {f.name} ({f.stat().st_size / 1024:.1f} KB)\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No DefInd parquet files found. You may need to run DefInd first:\")\n",
    "    print(\"   defind fetch-logs --registry nfpm --start-block 18000000 --end-block 18001000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Load and Examine DefInd Data\n",
    "if found_data:\n",
    "    print(\"\\nüìä Loading DefInd Data\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Load the first parquet file to examine structure\n",
    "    sample_file = found_data[0]\n",
    "    df = pd.read_parquet(sample_file)\n",
    "    \n",
    "    print(f\"Loaded {len(df)} rows from {sample_file.name}\")\n",
    "    print(f\"Columns: {list(df.columns)}\")\n",
    "    print(f\"\\nSample data:\")\n",
    "    print(df.head())\n",
    "    \n",
    "    # Check for NFPM events\n",
    "    if 'event_name' in df.columns:\n",
    "        event_counts = df['event_name'].value_counts()\n",
    "        print(f\"\\nEvent types found:\")\n",
    "        print(event_counts)\n",
    "        \n",
    "        # Focus on NFPM events for position tracking\n",
    "        nfpm_events = ['Mint', 'Burn', 'IncreaseLiquidity', 'DecreaseLiquidity', 'Collect', 'Transfer']\n",
    "        relevant_events = df[df['event_name'].isin(nfpm_events)] if 'event_name' in df.columns else df\n",
    "        print(f\"\\nRelevant NFPM events: {len(relevant_events)}\")\n",
    "    else:\n",
    "        relevant_events = df\n",
    "        print(\"\\nNo event_name column found, using all data\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Skipping data loading - no files found\")\n",
    "    relevant_events = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Create DuckDB Database from DefInd Data\n",
    "if not relevant_events.empty:\n",
    "    print(\"\\nüóÑÔ∏è Creating DuckDB Database\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Create DuckDB database\n",
    "    db_path = \"defind_pnl_test.duckdb\"\n",
    "    con = duckdb.connect(db_path)\n",
    "    \n",
    "    # Load all parquet files into DuckDB\n",
    "    print(\"Loading parquet files into DuckDB...\")\n",
    "    \n",
    "    all_data = []\n",
    "    for file in found_data:\n",
    "        df_temp = pd.read_parquet(file)\n",
    "        all_data.append(df_temp)\n",
    "    \n",
    "    combined_df = pd.concat(all_data, ignore_index=True)\n",
    "    print(f\"Combined data: {len(combined_df)} rows\")\n",
    "    \n",
    "    # Register with DuckDB\n",
    "    con.register(\"raw_events\", combined_df)\n",
    "    \n",
    "    # Create the tables expected by our PnL engine\n",
    "    print(\"Creating structured tables...\")\n",
    "    \n",
    "    # This is a simplified transformation - you'll need to adapt based on your actual DefInd schema\n",
    "    con.execute(\"\"\"\n",
    "        CREATE OR REPLACE TABLE pool_snapshots AS\n",
    "        SELECT \n",
    "            address as pool_id,\n",
    "            block_number,\n",
    "            block_timestamp as ts,\n",
    "            CAST(1000000000000000000000000 AS BIGINT) as sqrtPriceX96,  -- Mock value\n",
    "            0 as tick,  -- Mock value\n",
    "            CAST(0 AS BIGINT) as fg0_x128,  -- Mock value\n",
    "            CAST(0 AS BIGINT) as fg1_x128,  -- Mock value\n",
    "            CAST(0 AS BIGINT) as lower_fg0_out_x128,  -- Mock value\n",
    "            CAST(0 AS BIGINT) as lower_fg1_out_x128,  -- Mock value\n",
    "            CAST(0 AS BIGINT) as upper_fg0_out_x128,  -- Mock value\n",
    "            CAST(0 AS BIGINT) as upper_fg1_out_x128   -- Mock value\n",
    "        FROM raw_events \n",
    "        GROUP BY address, block_number, block_timestamp\n",
    "    \"\"\")\n",
    "    \n",
    "    con.execute(\"\"\"\n",
    "        CREATE OR REPLACE TABLE pool_prices AS\n",
    "        SELECT \n",
    "            address as pool_id,\n",
    "            block_timestamp as ts,\n",
    "            1.0 as token0_usd,    -- Mock USDC price\n",
    "            2000.0 as token1_usd  -- Mock WETH price\n",
    "        FROM raw_events \n",
    "        GROUP BY address, block_timestamp\n",
    "    \"\"\")\n",
    "    \n",
    "    print(\"‚úÖ DuckDB tables created\")\n",
    "    \n",
    "    # Show table sizes\n",
    "    snapshots_count = con.execute(\"SELECT COUNT(*) FROM pool_snapshots\").fetchone()[0]\n",
    "    prices_count = con.execute(\"SELECT COUNT(*) FROM pool_prices\").fetchone()[0]\n",
    "    print(f\"Pool snapshots: {snapshots_count}\")\n",
    "    print(f\"Pool prices: {prices_count}\")\n",
    "    \n",
    "    con.close()\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Skipping database creation - no data available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Test PnL Engine with Real Data Structure\n",
    "if not relevant_events.empty:\n",
    "    print(\"\\nüöÄ Testing PnL Engine with DefInd Data\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Load settings and create repository\n",
    "    settings = load_settings(duckdb_path=\"defind_pnl_test.duckdb\")\n",
    "    tables = {\n",
    "        \"pool_snapshots\": \"pool_snapshots\",\n",
    "        \"pool_prices\": \"pool_prices\",\n",
    "        \"position_lifecycle_active\": \"position_lifecycle_active\",  # We'll create this\n",
    "        \"position_lifecycle_closed\": \"position_lifecycle_closed\",\n",
    "        \"position_state_last\": \"position_state_last\"\n",
    "    }\n",
    "    \n",
    "    # Create mock position tables for testing\n",
    "    con = duckdb.connect(\"defind_pnl_test.duckdb\")\n",
    "    \n",
    "    # Create mock active positions\n",
    "    con.execute(\"\"\"\n",
    "        CREATE OR REPLACE TABLE position_lifecycle_active AS\n",
    "        SELECT \n",
    "            12345 as token_id,\n",
    "            (SELECT pool_id FROM pool_snapshots LIMIT 1) as pool_id,\n",
    "            -1000 as tick_lower,\n",
    "            1000 as tick_upper,\n",
    "            CURRENT_TIMESTAMP as open_time,\n",
    "            NULL as close_time,\n",
    "            0 as open_tick,\n",
    "            NULL as close_tick,\n",
    "            1000.0 as entry_amount0,\n",
    "            0.5 as entry_amount1,\n",
    "            1000000 as remaining_liquidity\n",
    "    \"\"\")\n",
    "    \n",
    "    con.execute(\"\"\"\n",
    "        CREATE OR REPLACE TABLE position_lifecycle_closed AS\n",
    "        SELECT * FROM position_lifecycle_active WHERE 1=0  -- Empty table\n",
    "    \"\"\")\n",
    "    \n",
    "    con.execute(\"\"\"\n",
    "        CREATE OR REPLACE TABLE position_state_last AS\n",
    "        SELECT \n",
    "            12345 as token_id,\n",
    "            1000000 as liquidity,\n",
    "            CAST(100000000000000000000000000000000000000 AS BIGINT) as fee_growth_inside0_last_x128,\n",
    "            CAST(50000000000000000000000000000000000000 AS BIGINT) as fee_growth_inside1_last_x128,\n",
    "            10 as tokens_owed0,\n",
    "            5 as tokens_owed1\n",
    "    \"\"\")\n",
    "    \n",
    "    con.close()\n",
    "    \n",
    "    print(\"‚úÖ Mock position tables created\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Skipping PnL engine test - no data available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Run Full Pipeline\n",
    "if not relevant_events.empty:\n",
    "    print(\"\\nüîÑ Running Full DefInd ‚Üí PnL Pipeline\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    try:\n",
    "        # Create repository and engine\n",
    "        repo = DuckDBRepository(\"defind_pnl_test.duckdb\", tables=tables)\n",
    "        engine = PNLEngine(pools=repo, positions=repo, prices=repo, rewards=repo, storage=repo)\n",
    "        \n",
    "        # Get a pool ID from our data\n",
    "        con = duckdb.connect(\"defind_pnl_test.duckdb\")\n",
    "        pool_id = con.execute(\"SELECT pool_id FROM pool_snapshots LIMIT 1\").fetchone()[0]\n",
    "        con.close()\n",
    "        \n",
    "        # Create pool object\n",
    "        pool = Pool(\n",
    "            id=pool_id,\n",
    "            token0=Token(\"0xa0b86a33e6441e6c7d3d0b4f4b1b8b8b8b8b8b8b\", \"USDC\", 6),\n",
    "            token1=Token(\"0xc02aaa39b223fe8d0a0e5c4f27ead9083c756cc2\", \"WETH\", 18),\n",
    "            fee=500\n",
    "        )\n",
    "        \n",
    "        # Get time range from data\n",
    "        con = duckdb.connect(\"defind_pnl_test.duckdb\")\n",
    "        time_range = con.execute(\"SELECT MIN(ts), MAX(ts) FROM pool_snapshots\").fetchone()\n",
    "        start_ts, end_ts = int(time_range[0]), int(time_range[1])\n",
    "        con.close()\n",
    "        \n",
    "        print(f\"Computing PnL for pool {pool_id}\")\n",
    "        print(f\"Time range: {datetime.fromtimestamp(start_ts)} to {datetime.fromtimestamp(end_ts)}\")\n",
    "        \n",
    "        # Compute intraday PnL\n",
    "        intraday_df = engine.compute_intraday(pool, start_ts=start_ts, end_ts=end_ts)\n",
    "        \n",
    "        print(f\"\\n‚úÖ Computed {len(intraday_df)} intraday PnL rows\")\n",
    "        \n",
    "        if not intraday_df.empty:\n",
    "            print(\"\\nIntraday PnL columns:\")\n",
    "            print(list(intraday_df.columns))\n",
    "            \n",
    "            print(\"\\nSample intraday data:\")\n",
    "            print(intraday_df[['token_id', 'timestamp', 'value_total_usd', 'il_usd']].head())\n",
    "            \n",
    "            # Aggregate to daily\n",
    "            daily_df = to_daily(intraday_df)\n",
    "            print(f\"\\n‚úÖ Aggregated to {len(daily_df)} daily rows\")\n",
    "            \n",
    "            if not daily_df.empty:\n",
    "                print(\"\\nDaily aggregated data:\")\n",
    "                print(daily_df)\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è No intraday data generated - check position/snapshot alignment\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error running pipeline: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Cannot run pipeline - no DefInd data found\")\n",
    "    print(\"\\nTo get DefInd data, run:\")\n",
    "    print(\"  cd ../src/defind\")\n",
    "    print(\"  defind fetch-logs --registry nfpm --start-block 18000000 --end-block 18001000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Analysis and Visualization\n",
    "print(\"\\nüìà Analysis Summary\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "if 'intraday_df' in locals() and not intraday_df.empty:\n",
    "    print(\"\\nüéâ Full Pipeline Success!\")\n",
    "    print(\"\\nPipeline Summary:\")\n",
    "    print(f\"  ‚Ä¢ DefInd data files processed: {len(found_data) if found_data else 0}\")\n",
    "    print(f\"  ‚Ä¢ Total events loaded: {len(combined_df) if 'combined_df' in locals() else 0}\")\n",
    "    print(f\"  ‚Ä¢ Pool snapshots created: {snapshots_count if 'snapshots_count' in locals() else 0}\")\n",
    "    print(f\"  ‚Ä¢ Price points created: {prices_count if 'prices_count' in locals() else 0}\")\n",
    "    print(f\"  ‚Ä¢ Intraday PnL rows: {len(intraday_df)}\")\n",
    "    print(f\"  ‚Ä¢ Daily PnL rows: {len(daily_df) if 'daily_df' in locals() else 0}\")\n",
    "    \n",
    "    print(\"\\n‚úÖ The DefInd ‚Üí PnL pipeline is working!\")\n",
    "    \n",
    "    # Show key metrics\n",
    "    if len(intraday_df) > 0:\n",
    "        total_value = intraday_df['value_total_usd'].iloc[-1]\n",
    "        total_il = intraday_df['il_usd'].iloc[-1]\n",
    "        print(f\"\\nFinal Position Metrics:\")\n",
    "        print(f\"  ‚Ä¢ Total Value: ${total_value:.2f}\")\n",
    "        print(f\"  ‚Ä¢ Impermanent Loss: ${total_il:.2f}\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Pipeline incomplete - missing DefInd data\")\n",
    "    print(\"\\nNext steps:\")\n",
    "    print(\"1. Run DefInd to index some blockchain events\")\n",
    "    print(\"2. Re-run this notebook to see the full pipeline\")\n",
    "    print(\"3. The PnL engine is ready and waiting for data!\")\n",
    "\n",
    "print(\"\\nüöÄ DefInd + Define integration complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
